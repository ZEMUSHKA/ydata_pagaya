{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0Tv3jKe4syG"
   },
   "outputs": [],
   "source": [
    "# Checklist:\n",
    "# AWS emr-5.29.0\n",
    "# MASTER r5d.8xlarge 1x, no EBS\n",
    "# CORE r5d.8xlarge 4x, no EBS\n",
    "# Custom bootstrap action: s3://ydatazian/bootstrap.sh\n",
    "# Allow ssh in master node security group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CGHGHkqE4rJw",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3duPlU1fjxPz"
   },
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u566smRWkDOS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NameNode: http://ec2-3-238-44-219.compute-1.amazonaws.com:50070\n",
      "YARN: http://ec2-3-238-44-219.compute-1.amazonaws.com:8088\n",
      "Spark UI: http://ec2-3-238-44-219.compute-1.amazonaws.com:20888/proxy/application_1636529659796_0001\n"
     ]
    }
   ],
   "source": [
    "# connect, context, session\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import spark_utils\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"yarn\", \"My App\", conf=spark_utils.get_spark_conf())\n",
    "\n",
    "spark_utils.print_ui_links()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BxHvUF7qpEHO"
   },
   "outputs": [],
   "source": [
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCrxvDnx47f2"
   },
   "source": [
    "## HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-tZacCcy49Lv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                                   Size     Used  Available  Use%\r\n",
      "hdfs://ip-172-31-7-108.ec2.internal:8020  547.5 G  222.3 M    546.0 G    0%\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1m8mBXzZ4-kB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxrwxrwt   - hdfs hadoop          0 2021-11-10 07:34 /tmp\r\n",
      "drwxr-xr-x   - hdfs hadoop          0 2021-11-10 07:34 /user\r\n",
      "drwxr-xr-x   - hdfs hadoop          0 2021-11-10 07:34 /var\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrcHwAaQjxam"
   },
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bakU6PKuj-0s"
   },
   "source": [
    "RDD (Resilient Distributed Datasets) - base data block of Spark. The system takes care about parts of the data and it's manipulations on distributed system. It could be treated as ordered sequence of rows (commonly key-value pairs like in MapReduce, but could be any arbitrary data).\n",
    "\n",
    "RDDs are immutable. You get new RDD by making operations on initial RDD.\n",
    "\n",
    "There is two kinds of operations on RDD: *actions* and *transformations*.\n",
    "\n",
    "Transformations are not applied instantly, they are stacked in operations order.\n",
    "\n",
    "Actions are used to materialize transformations (so the data is actually transformed on cluster).\n",
    "\n",
    "Documentation: https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PLjBvt5YmiuU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create simple RDD first\n",
    "rdd = sc.parallelize(range(10))\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPC6YOl-lLM3"
   },
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Y2Xhvx9lj8YB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() # gather data into python, be careful, loads data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FoCGZgLimxcA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count() # returns count of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YLWX_3kymxfU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first() # get first element of RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KqYXt8cdrMC5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2) # get first N=2 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IcDLdYsSrPs3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mean() # mean of RDD's values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8fhwaR-OrTZI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[27] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can create RDD with text data\n",
    "rdd = sc.parallelize([\"one\", \"two\"] * 1000)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_aZFNOvprjNa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'two']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() # get RDD values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QVBjW4qirmGE"
   },
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile(\"/tmp_text2.txt\")  # save RDD into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "U9Lim1dJrsYo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 501 items\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-11-10 09:36 /tmp_text2.txt/_SUCCESS\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00000\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00001\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00002\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00003\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00004\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00005\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00006\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00007\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00008\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00009\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00010\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00011\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00012\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00013\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00014\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00015\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00016\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00017\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00018\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00019\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00020\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00021\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00022\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00023\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00024\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00025\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00026\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00027\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00028\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00029\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00030\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00031\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00032\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00033\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00034\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00035\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00036\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00037\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00038\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00039\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00040\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00041\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00042\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00043\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00044\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00045\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00046\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00047\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00048\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00049\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00050\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00051\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00052\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00053\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00054\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00055\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00056\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00057\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00058\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00059\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00060\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00061\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00062\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00063\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00064\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00065\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00066\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00067\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00068\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00069\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00070\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00071\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00072\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00073\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00074\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00075\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00076\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00077\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00078\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00079\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00080\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00081\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00082\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00083\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00084\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00085\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00086\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00087\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00088\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00089\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00090\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00091\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00092\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00093\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00094\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00095\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00096\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00097\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00098\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00099\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00100\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00101\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00102\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00103\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00104\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00105\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00106\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00107\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00108\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00109\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00110\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00111\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00112\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00113\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00114\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00115\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00116\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00117\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00118\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00119\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00120\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00121\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00122\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00123\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00124\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00125\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00126\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00127\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00128\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00129\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00130\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00131\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00132\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00133\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00134\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00135\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00136\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00137\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00138\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00139\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00140\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00141\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00142\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00143\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00144\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00145\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00146\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00147\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00148\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00149\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00150\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00151\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00152\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00153\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00154\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00155\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00156\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00157\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00158\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00159\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00160\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00161\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00162\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00163\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00164\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00165\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00166\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00167\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00168\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00169\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00170\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00171\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00172\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00173\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00174\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00175\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00176\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00177\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00178\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00179\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00180\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00181\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00182\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00183\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00184\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00185\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00186\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00187\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00188\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00189\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00190\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00191\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00192\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00193\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00194\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00195\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00196\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00197\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00198\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00199\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00200\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00201\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00202\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00203\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00204\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00205\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00206\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00207\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00208\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00209\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00210\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00211\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00212\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00213\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00214\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00215\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00216\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00217\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00218\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00219\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00220\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00221\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00222\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00223\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00224\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00225\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00226\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00227\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00228\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00229\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00230\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00231\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00232\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00233\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00234\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00235\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00236\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00237\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00238\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00239\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00240\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00241\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00242\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00243\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00244\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00245\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00246\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00247\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00248\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00249\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00250\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00251\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00252\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00253\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00254\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00255\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00256\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00257\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00258\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00259\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00260\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00261\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00262\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00263\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00264\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00265\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00266\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00267\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00268\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00269\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00270\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00271\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00272\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00273\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00274\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00275\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00276\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00277\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00278\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00279\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00280\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00281\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00282\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00283\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00284\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00285\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00286\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00287\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00288\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00289\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00290\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00291\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00292\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00293\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00294\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00295\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00296\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00297\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00298\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00299\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00300\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00301\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00302\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00303\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00304\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00305\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00306\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00307\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00308\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00309\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00310\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00311\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00312\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00313\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00314\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00315\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00316\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00317\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00318\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00319\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00320\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00321\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00322\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00323\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00324\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00325\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00326\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00327\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00328\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00329\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00330\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00331\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00332\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00333\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00334\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00335\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00336\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00337\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00338\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00339\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00340\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00341\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00342\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00343\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00344\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00345\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00346\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00347\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00348\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00349\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00350\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00351\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00352\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00353\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00354\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00355\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00356\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00357\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00358\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00359\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00360\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00361\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00362\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00363\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00364\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:35 /tmp_text2.txt/part-00365\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00366\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00367\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00368\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00369\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00370\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00371\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00372\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00373\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00374\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00375\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00376\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00377\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00378\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00379\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00380\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00381\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00382\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00383\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00384\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00385\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00386\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00387\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00388\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00389\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00390\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00391\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00392\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00393\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00394\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00395\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00396\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00397\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00398\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00399\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00400\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00401\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00402\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00403\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00404\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00405\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00406\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00407\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00408\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00409\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00410\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00411\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00412\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00413\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00414\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00415\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00416\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00417\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00418\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00419\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00420\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00421\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00422\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00423\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00424\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00425\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00426\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00427\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00428\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00429\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00430\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00431\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00432\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00433\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00434\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00435\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00436\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00437\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00438\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00439\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00440\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00441\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00442\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00443\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00444\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00445\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00446\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00447\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00448\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00449\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00450\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00451\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00452\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00453\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00454\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00455\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00456\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00457\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00458\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00459\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00460\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00461\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00462\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00463\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00464\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00465\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00466\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00467\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00468\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00469\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00470\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00471\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00472\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00473\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00474\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00475\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00476\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00477\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00478\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00479\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00480\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00481\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00482\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00483\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00484\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00485\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00486\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00487\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00488\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00489\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00490\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00491\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00492\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00493\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00494\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00495\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00496\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00497\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00498\n",
      "-rw-r--r--   1 hadoop hadoop         16 2021-11-10 09:36 /tmp_text2.txt/part-00499\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /tmp_text2.txt # parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "eIKRO5D3rzeC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n",
      "one\n",
      "two\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -cat /tmp_text2.txt/* # actual data from parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([{1: 1}, {2: 2}] * 1000, 10)\n",
    "rdd.saveAsTextFile(\"/test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n",
      "{1: 1}\n",
      "{2: 2}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -cat /test2/* # actual data from parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 items\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-11-10 09:39 /test2/_SUCCESS\n",
      "-rw-r--r--   1 hadoop hadoop       1400 2021-11-10 09:39 /test2/part-00000\n",
      "-rw-r--r--   1 hadoop hadoop       1400 2021-11-10 09:39 /test2/part-00001\n",
      "-rw-r--r--   1 hadoop hadoop       1400 2021-11-10 09:39 /test2/part-00002\n",
      "-rw-r--r--   1 hadoop hadoop       1400 2021-11-10 09:39 /test2/part-00003\n",
      "-rw-r--r--   1 hadoop hadoop       1400 2021-11-10 09:39 /test2/part-00004\n",
      "-rw-r--r--   1 hadoop hadoop       1400 2021-11-10 09:39 /test2/part-00005\n",
      "-rw-r--r--   1 hadoop hadoop       1400 2021-11-10 09:39 /test2/part-00006\n",
      "-rw-r--r--   1 hadoop hadoop       1400 2021-11-10 09:39 /test2/part-00007\n",
      "-rw-r--r--   1 hadoop hadoop       1400 2021-11-10 09:39 /test2/part-00008\n",
      "-rw-r--r--   1 hadoop hadoop       1400 2021-11-10 09:39 /test2/part-00009\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kk4wsCJ1lOUd"
   },
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "iruV4O0hlObo"
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(20), 10) # RDD from range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "2L3-6lOpmln7"
   },
   "outputs": [],
   "source": [
    "# New RDD with \"square + 1\" transformation by two map operations;\n",
    "# Map operations are similar to those from MapReduce,\n",
    "# the difference - given map functions are applied to each element of rdd:\n",
    "squares = rdd.map(lambda x: x**2).map(lambda x: x + 1)\n",
    "\n",
    "# IMPORTANT NOTE - nothing is calculated right now,\n",
    "# `squares` now only represents sequence for new RDD over initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[60] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "-KcdXAyZmyMG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squares.first()\n",
    "\n",
    "# Now we applied Action, so the map transformations are run\n",
    "# But not all data is calculated, Spark optimized that for us\n",
    "# and only the value for first row was calculated and returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "DyY3JBSmt09P",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 5,\n",
       " 10,\n",
       " 17,\n",
       " 26,\n",
       " 37,\n",
       " 50,\n",
       " 65,\n",
       " 82,\n",
       " 101,\n",
       " 122,\n",
       " 145,\n",
       " 170,\n",
       " 197,\n",
       " 226,\n",
       " 257,\n",
       " 290,\n",
       " 325,\n",
       " 362]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squares.collect() # get all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "IxQRwjxVvrS_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(squares.sample(False, 0.5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "j927zl3DwD5q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 2, 3]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squares.flatMap(lambda x: [x, x+1, x+2]).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "1OrvtNaJwG3H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[362]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    squares\n",
    "    .takeOrdered(1, lambda x: -x) # top 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "8ivcPWBTfglT",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    rdd\n",
    "    .filter(lambda x: x % 2)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7QmOvTlmyWj"
   },
   "source": [
    "### MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "tZpXudV_mz12"
   },
   "outputs": [],
   "source": [
    "# step by step MapReduce emulation:\n",
    "rdd = sc.parallelize([\"this is text\", \"some more text\"], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "x862XjL0m0a-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 1), ('is', 1), ('text', 1), ('some', 1), ('more', 1), ('text', 1)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    rdd\n",
    "    .flatMap(lambda x: [(w, 1) for w in x.split()])\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "GhOYxLaR2Qr_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we gonna use first iteration often, to save time we can cache the result:\n",
    "words = rdd.flatMap(lambda x: [(w, 1) for w in x.split()]).cache()\n",
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 1), ('is', 1), ('text', 1), ('some', 1), ('more', 1), ('text', 1)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42qUIO7SgXil"
   },
   "source": [
    "**PairRDD**\n",
    "\n",
    "If you have a tuple of length 2 as your RDD data type, you can use *ByKey operations on your RDD, with first value of tuple being the key and second being the value. Let's create such RDD.\n",
    "\n",
    "We want to aggregate data by key (word), we are able to do it with `groupByKey` method, it will produce values iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "OXWh_qA0m0uH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', <pyspark.resultiterable.ResultIterable at 0x7f84ebc3d6a0>),\n",
       " ('more', <pyspark.resultiterable.ResultIterable at 0x7f84ebc3d3c8>),\n",
       " ('this', <pyspark.resultiterable.ResultIterable at 0x7f84ebc3d438>),\n",
       " ('some', <pyspark.resultiterable.ResultIterable at 0x7f84ebc3d9b0>),\n",
       " ('is', <pyspark.resultiterable.ResultIterable at 0x7f84ebc3df60>)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    words\n",
    "    .groupByKey()\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJeahVF0glWe"
   },
   "source": [
    "And of course we can use any function in map, not just lambdas,\n",
    "\n",
    "with regular `map` function you can change key/value, create complex keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "nw5USXbkxvOe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', [1, 1]), ('more', [1]), ('this', [1]), ('some', [1]), ('is', [1])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapToList(x):\n",
    "    return x[0], list(x[1])\n",
    "\n",
    "data = (\n",
    "    words\n",
    "    .groupByKey()\n",
    "    .map(mapToList)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEtvZG4biDVl"
   },
   "source": [
    "We may use `.mapValues` method to manipulate only with values and leave keys intact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "hnXI092aiKCq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 2), ('more', 1), ('this', 1), ('some', 1), ('is', 1)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapValuesToLen(x):\n",
    "    return len(x)\n",
    "\n",
    "(\n",
    "    words\n",
    "    .groupByKey()\n",
    "    .mapValues(mapValuesToLen)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "g6dQHKHsxvQq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 2), ('more', 1), ('this', 1), ('some', 1), ('is', 1)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    words\n",
    "    .groupByKey()\n",
    "    .map(lambda x: (x[0], len(x[1])))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiRdwghKhw5w"
   },
   "source": [
    "Another way to manipulate values grouped by key is reduce performed by `reduceByKey` operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "zSL9x0vpxvTB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 2), ('more', 1), ('this', 1), ('some', 1), ('is', 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    words # -> (word, cnt=1)\n",
    "    .reduceByKey(lambda a, b: a + b) # -> (word, sum(cnt))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "lK4YatXIy-zx"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 61.0 failed 4 times, most recent failure: Lost task 4.3 in stage 61.0 (TID 8741, ip-172-31-10-0.ec2.internal, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-81-fda08014ba43>\", line 5, in <lambda>\n  File \"<ipython-input-81-fda08014ba43>\", line 5, in <listcomp>\nTypeError: object of type 'int' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-81-fda08014ba43>\", line 5, in <lambda>\n  File \"<ipython-input-81-fda08014ba43>\", line 5, in <listcomp>\nTypeError: object of type 'int' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-fda08014ba43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# len from int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# -> (word, sum(cnt))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 61.0 failed 4 times, most recent failure: Lost task 4.3 in stage 61.0 (TID 8741, ip-172-31-10-0.ec2.internal, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-81-fda08014ba43>\", line 5, in <lambda>\n  File \"<ipython-input-81-fda08014ba43>\", line 5, in <listcomp>\nTypeError: object of type 'int' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor267.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1636529659796_0001/container_1636529659796_0001_01_000258/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-81-fda08014ba43>\", line 5, in <lambda>\n  File \"<ipython-input-81-fda08014ba43>\", line 5, in <listcomp>\nTypeError: object of type 'int' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# how errors look like:\n",
    "(\n",
    "    rdd\n",
    "    .flatMap(lambda x: [(w, len(1)) for w in x.split()]) # len from int\n",
    "    .reduceByKey(lambda a, b: a + b) # -> (word, sum(cnt))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Q1OO-vOyY6L"
   },
   "source": [
    "## Broadcast and accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "RjypNMTVyYOp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[123] at RDD at PythonRDD.scala:53\n",
      "[(0, 1), (1, 1), (2, 2)]\n",
      "errors: 3\n"
     ]
    }
   ],
   "source": [
    "bc = sc.broadcast({\"this\": 0, \"is\": 1, \"text\": 2})  # read-only\n",
    "errors = sc.accumulator(0)  # write-only\n",
    "\n",
    "# x - \"this is text\"\n",
    "def mapper(x):\n",
    "    global errors\n",
    "    for w in x.split():\n",
    "        if w in bc.value:\n",
    "            yield (bc.value[w], 1)\n",
    "        else:\n",
    "            errors += 1\n",
    "\n",
    "rdd = (\n",
    "    sc\n",
    "   .parallelize([\"this is text too\", \"text too too\"], 10)\n",
    "   .flatMap(mapper)\n",
    "   .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "print(rdd)\n",
    "print(rdd.collect())\n",
    "print(\"errors:\", errors.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHR7Vzn_jxlF"
   },
   "source": [
    "## DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDbdsczvj_i8"
   },
   "source": [
    "RDD is much better and useful than plain MapReduce, but Spark can do even more!\n",
    "Spark DataFrame is table structure over RDDs and can be treated as pandas on steroids.\n",
    "\n",
    "It allows us to perform structured queries and benefit from it. One way is to perform SQL-styled queries (will discuss on next lesson) and another is DataFrame API.\n",
    "\n",
    "Documentation: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "UD21BVEqj3zA"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "EpYvw_HymmTQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('a', 2), ('b', 3), ('b', 4)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"a\", 2), (\"b\", 3), (\"b\", 4)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "i4mxAmRemmb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  2|\n",
      "|  b|  3|\n",
      "|  b|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = se.createDataFrame(rdd)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='a', _2=1), Row(_1='a', _2=2), Row(_1='b', _2=3), Row(_1='b', _2=4)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df -> rdd\n",
    "df.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('a', 2), ('b', 3), ('b', 4)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd -> df\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "FGJOiQS_2G_-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col_one: string (nullable = true)\n",
      " |-- col_two: long (nullable = true)\n",
      "\n",
      "+-------+-------+\n",
      "|col_one|col_two|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      a|      2|\n",
      "|      b|      3|\n",
      "|      b|      4|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = se.createDataFrame(\n",
    "    rdd.map(lambda x: Row(col_one=x[0], col_two=x[1]))\n",
    ")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "u1RymT0N7n8z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|col_one|\n",
      "+-------+\n",
      "|      a|\n",
      "|      a|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['col_one']).limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|col_one|\n",
      "+-------+\n",
      "|      b|\n",
      "|      a|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['col_one']).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "wwkvZly3774s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'a']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when you need it in Python\n",
    "df.select(['col_one']).distinct().rdd.map(lambda x: x.col_one).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docs: https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "D_QIAytR70kc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|col_one|col_two|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      a|      2|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "(\n",
    "    df\n",
    "    .select(['col_one', 'col_two'])\n",
    "    .where(F.col('col_one') == 'a')\n",
    "    .limit(2)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|col_one|col_two|\n",
      "+-------+-------+\n",
      "|      a|      2|\n",
      "|      a|      1|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .select(['col_one', 'col_two'])\n",
    "    .where(df.col_one == 'a')\n",
    "    .limit(2)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|col_one|col_two|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      a|      2|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the same thing\n",
    "df.registerTempTable(\"table\")\n",
    "se.sql(\"select col_one, col_two from table where col_one = 'a' limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|col_one|col_two|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      a|      2|\n",
      "|      b|      3|\n",
      "|      b|      4|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "rNp94hJnjHqW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|col_one|col_two|col_two_float|\n",
      "+-------+-------+-------------+\n",
      "|      a|      1|          1.0|\n",
      "|      a|      2|          2.0|\n",
      "|      b|      3|          3.0|\n",
      "|      b|      4|          4.0|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select('*', df['col_two'].cast('float').alias('col_two_float'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|col_one|col_two|col_two_float|\n",
      "+-------+-------+-------------+\n",
      "|      a|      1|          1.0|\n",
      "|      a|      2|          2.0|\n",
      "|      b|      3|          3.0|\n",
      "|      b|      4|          4.0|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the same thing\n",
    "se.sql(\"\"\"\n",
    "select *, cast(col_two as float) as col_two_float\n",
    "from table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "fMgcNpPmjTUM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|col_one|col_two_square|\n",
      "+-------+--------------+\n",
      "|      b|          16.0|\n",
      "|      b|           9.0|\n",
      "|      a|           4.0|\n",
      "|      a|           1.0|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "square_df = df.select('col_one', (df['col_two_float'] * df['col_two_float']).alias('col_two_square'))\n",
    "square_df.orderBy('col_two_square', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|col_one|col_two|col_two_float|\n",
      "+-------+-------+-------------+\n",
      "|      a|      1|          1.0|\n",
      "|      a|      2|          2.0|\n",
      "|      b|      3|          3.0|\n",
      "|      b|      4|          4.0|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "6TO3OLMN7W51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|col_one|col_two_list|\n",
      "+-------+------------+\n",
      "|      b|      [3, 4]|\n",
      "|      a|      [1, 2]|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "(\n",
    "  df\n",
    "  .groupby('col_one')\n",
    "  .agg(F.collect_list(\"col_two\").alias(\"col_two_list\"))\n",
    "  .select(['col_one', 'col_two_list'])\n",
    "  .limit(10)\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "lc31NQ0x2Lv-"
   },
   "source": [
    "# convertable to Pandas\n",
    "pandas_df = df.toPandas()\n",
    "pandas_df\n",
    "\n",
    "# load from Pandas\n",
    "df = se.createDataFrame(pandas_df)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi3keNw6mteI"
   },
   "source": [
    "## Data formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "Mt8434r9mvr2"
   },
   "outputs": [],
   "source": [
    "# We may want to operate with not just plain text, but something more complex\n",
    "# For example, Parquet - it can be useful for huge datasets for faster calcs\n",
    "df.write.save(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwx------   - hadoop hadoop          0 2021-11-10 09:25 /user/hadoop/.sparkStaging/application_1636529659796_0001\r\n",
      "Found 6 items\r\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-11-10 10:31 /user/hadoop/data.parquet/_SUCCESS\r\n",
      "-rw-r--r--   1 hadoop hadoop        450 2021-11-10 10:31 /user/hadoop/data.parquet/part-00000-2586d0f6-6e36-4485-8b3f-04cc1f4ebaec-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop        859 2021-11-10 10:31 /user/hadoop/data.parquet/part-00124-2586d0f6-6e36-4485-8b3f-04cc1f4ebaec-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop        859 2021-11-10 10:31 /user/hadoop/data.parquet/part-00249-2586d0f6-6e36-4485-8b3f-04cc1f4ebaec-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop        859 2021-11-10 10:31 /user/hadoop/data.parquet/part-00374-2586d0f6-6e36-4485-8b3f-04cc1f4ebaec-c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop        859 2021-11-10 10:31 /user/hadoop/data.parquet/part-00499-2586d0f6-6e36-4485-8b3f-04cc1f4ebaec-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/hadoop/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "zxDJLG6pmvwf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col_one='a', col_two=1, col_two_float=1.0),\n",
       " Row(col_one='a', col_two=2, col_two_float=2.0),\n",
       " Row(col_one='b', col_two=3, col_two_float=3.0),\n",
       " Row(col_one='b', col_two=4, col_two_float=4.0)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = se.read.parquet(\"data.parquet\")\n",
    "data.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqXylvo5kJa-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_seminar (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
