{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NameNode: http://ec2-3-235-150-104.compute-1.amazonaws.com:50070\n",
      "YARN: http://ec2-3-235-150-104.compute-1.amazonaws.com:8088\n",
      "Spark UI: http://ec2-3-235-150-104.compute-1.amazonaws.com:20888/proxy/application_1620307932889_0002\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import spark_utils\n",
    "spark_utils.print_ui_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS config\n",
    "\n",
    "https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    <name>dfs.datanode.data.dir</name>\r\n",
      "    <value>file:///mnt/hdfs</value>\r\n",
      "--\r\n",
      "    <name>dfs.namenode.name.dir</name>\r\n",
      "    <value>file:///mnt/namenode</value>\r\n",
      "--\r\n",
      "    <name>dfs.namenode.replication.max-streams</name>\r\n",
      "    <value>20</value>\r\n",
      "--\r\n",
      "    <name>dfs.namenode.replication.max-streams-hard-limit</name>\r\n",
      "    <value>40</value>\r\n",
      "--\r\n",
      "    <name>dfs.namenode.replication.work.multiplier.per.iteration</name>\r\n",
      "    <value>10</value>\r\n",
      "--\r\n",
      "    <name>dfs.replication</name>\r\n",
      "    <value>1</value>\r\n",
      "--\r\n",
      "    <name>dfs.name.dir</name>\r\n",
      "    <value>/mnt/namenode</value>\r\n",
      "--\r\n",
      "    <name>dfs.data.dir</name>\r\n",
      "    <value>/mnt/hdfs</value>\r\n"
     ]
    }
   ],
   "source": [
    "! cat /etc/hadoop/conf/hdfs-site.xml | egrep -A 1 \".dir|.replication\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\r\n",
      "\t[-appendToFile <localsrc> ... <dst>]\r\n",
      "\t[-cat [-ignoreCrc] <src> ...]\r\n",
      "\t[-checksum <src> ...]\r\n",
      "\t[-chgrp [-R] GROUP PATH...]\r\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\r\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\r\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\r\n",
      "\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\r\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] <path> ...]\r\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\r\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\r\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\r\n",
      "\t[-df [-h] [<path> ...]]\r\n",
      "\t[-du [-s] [-h] [-x] <path> ...]\r\n",
      "\t[-expunge]\r\n",
      "\t[-find <path> ... <expression> ...]\r\n",
      "\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\r\n",
      "\t[-getfacl [-R] <path>]\r\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\r\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\r\n",
      "\t[-help [cmd ...]]\r\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> ...]]\r\n",
      "\t[-mkdir [-p] <path> ...]\r\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\r\n",
      "\t[-moveToLocal <src> <localdst>]\r\n",
      "\t[-mv <src> ... <dst>]\r\n",
      "\t[-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\r\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\r\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\r\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\r\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\r\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\r\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\r\n",
      "\t[-stat [format] <path> ...]\r\n",
      "\t[-tail [-f] <file>]\r\n",
      "\t[-test -[defsz] <path>]\r\n",
      "\t[-text [-ignoreCrc] <src> ...]\r\n",
      "\t[-touchz <path> ...]\r\n",
      "\t[-truncate [-w] <length> <path> ...]\r\n",
      "\t[-usage [cmd ...]]\r\n",
      "\r\n",
      "Generic options supported are\r\n",
      "-conf <configuration file>     specify an application configuration file\r\n",
      "-D <property=value>            use value for given property\r\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\r\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\r\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\r\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\r\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\r\n",
      "\r\n",
      "The general command line syntax is\r\n",
      "command [genericOptions] [commandOptions]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy files from/to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"Test file\" > test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 68\r\n",
      "-rw-rw-r-- 1 hadoop hadoop   149 May  6 21:23 generate.py\r\n",
      "-rw-rw-r-- 1 hadoop hadoop 24840 May  6 21:52 hdfs-basics.ipynb\r\n",
      "-rw-rw-r-- 1 hadoop hadoop 27369 May  6 21:12 mapreduce-wordcount.ipynb\r\n",
      "-rw-r--r-- 1 hadoop hadoop    10 May  6 21:18 test2.txt\r\n",
      "-rw-rw-r-- 1 hadoop hadoop    10 May  6 21:53 test.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/test.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -copyFromLocal test.txt /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\r\n",
      "-rw-r--r--   1 hadoop hadoop  134217721 2021-05-06 21:24 /128.txt\r\n",
      "-rw-r--r--   1 hadoop hadoop  268435451 2021-05-06 21:24 /256.txt\r\n",
      "-rw-r--r--   2 hadoop hadoop         10 2021-05-06 21:18 /test.txt\r\n",
      "-rw-r--r--   1 hadoop hadoop          8 2021-05-06 21:19 /test3.txt\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2021-05-06 13:32 /tmp\r\n",
      "drwxr-xr-x   - hdfs   hadoop          0 2021-05-06 13:32 /user\r\n",
      "drwxr-xr-x   - hdfs   hadoop          0 2021-05-06 13:32 /var\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyToLocal: `test2.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -copyToLocal /test.txt test2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file\r\n"
     ]
    }
   ],
   "source": [
    "! cat test2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming from/to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /test.txt | wc -w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/test3.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! echo \"1 2 3 4\" | hadoop fs -copyFromLocal - /test3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /test3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    <name>dfs.replication</name>\r\n",
      "    <value>1</value>\r\n"
     ]
    }
   ],
   "source": [
    "! cat /etc/hadoop/conf/hdfs-site.xml | grep -A 1 \"dfs.replication\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -stat %r /test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication 2 set: /test.txt\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -setrep 2 /test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -stat %r /test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default block size is 128 MB\n",
    "! cat /etc/hadoop/conf/hdfs-site.xml | grep -A 1 \"block\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generate.py\n"
     ]
    }
   ],
   "source": [
    "%%file generate.py\n",
    "import sys\n",
    "\n",
    "megabytes = int(sys.argv[1])\n",
    "string = \"{}M\\n\".format(megabytes)\n",
    "count = 1024 * 1024 * megabytes // len(string) - 1\n",
    "print(string * count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1M\r\n",
      "1M\r\n",
      "1M\r\n",
      "1M\r\n",
      "1M\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"./generate.py\", line 6, in <module>\r\n",
      "    print(string * count)\r\n",
      "IOError: [Errno 32] Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! python3 ./generate.py 1 | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /128.txt\r\n",
      "Deleted /256.txt\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm /128.txt /256.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 ./generate.py 128 | hadoop fs -copyFromLocal - /128.txt\n",
    "\n",
    "! python3 ./generate.py 256 | hadoop fs -copyFromLocal - /256.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://ip-172-31-13-255.ec2.internal:50070/fsck?ugi=hadoop&files=1&blocks=1&path=%2F\n",
      "FSCK started by hadoop (auth:SIMPLE) from /172.31.13.255 for path / at Thu May 06 21:54:23 UTC 2021\n",
      "/ <dir>\n",
      "/128.txt 134217721 bytes, 1 block(s):  OK\n",
      "0. BP-1596529054-172.31.13.255-1620307919098:blk_1073741851_1027 len=134217721 Live_repl=1\n",
      "\n",
      "/256.txt 268435451 bytes, 2 block(s):  OK\n",
      "0. BP-1596529054-172.31.13.255-1620307919098:blk_1073741852_1028 len=134217728 Live_repl=1\n",
      "1. BP-1596529054-172.31.13.255-1620307919098:blk_1073741853_1029 len=134217723 Live_repl=1\n",
      "\n",
      "/test.txt 10 bytes, 1 block(s):  OK\n",
      "0. BP-1596529054-172.31.13.255-1620307919098:blk_1073741846_1022 len=10 Live_repl=2\n",
      "\n",
      "/test3.txt 8 bytes, 1 block(s):  OK\n",
      "0. BP-1596529054-172.31.13.255-1620307919098:blk_1073741847_1023 len=8 Live_repl=1\n",
      "\n",
      "/tmp <dir>\n",
      "/tmp/hadoop-yarn <dir>\n",
      "/tmp/hadoop-yarn/staging <dir>\n",
      "/tmp/hadoop-yarn/staging/history <dir>\n",
      "/tmp/hadoop-yarn/staging/history/done <dir>\n",
      "/tmp/hadoop-yarn/staging/history/done_intermediate <dir>\n",
      "/user <dir>\n",
      "/user/hadoop <dir>\n",
      "/user/hadoop/.sparkStaging <dir>\n",
      "/user/hadoop/.sparkStaging/application_1620307932889_0002 <dir>\n",
      "/user/hadoop/.sparkStaging/application_1620307932889_0002/__spark_conf__.zip 238682 bytes, 1 block(s):  OK\n",
      "0. BP-1596529054-172.31.13.255-1620307919098:blk_1073741844_1020 len=238682 Live_repl=1\n",
      "\n",
      "/user/hadoop/.sparkStaging/application_1620307932889_0002/__spark_libs__2227692372283133473.zip 230327364 bytes, 2 block(s):  OK\n",
      "0. BP-1596529054-172.31.13.255-1620307919098:blk_1073741840_1016 len=134217728 Live_repl=1\n",
      "1. BP-1596529054-172.31.13.255-1620307919098:blk_1073741841_1017 len=96109636 Live_repl=1\n",
      "\n",
      "/user/hadoop/.sparkStaging/application_1620307932889_0002/py4j-0.10.7-src.zip 42437 bytes, 1 block(s):  OK\n",
      "0. BP-1596529054-172.31.13.255-1620307919098:blk_1073741843_1019 len=42437 Live_repl=1\n",
      "\n",
      "/user/hadoop/.sparkStaging/application_1620307932889_0002/pyspark.zip 594786 bytes, 1 block(s):  OK\n",
      "0. BP-1596529054-172.31.13.255-1620307919098:blk_1073741842_1018 len=594786 Live_repl=1\n",
      "\n",
      "/user/history <dir>\n",
      "/user/livy <dir>\n",
      "/user/livy/.sparkStaging <dir>\n",
      "/user/root <dir>\n",
      "/user/spark <dir>\n",
      "/user/spark/warehouse <dir>\n",
      "/var <dir>\n",
      "/var/log <dir>\n",
      "/var/log/hadoop-yarn <dir>\n",
      "/var/log/hadoop-yarn/apps <dir>\n",
      "/var/log/hadoop-yarn/apps/hadoop <dir>\n",
      "/var/log/hadoop-yarn/apps/hadoop/logs <dir>\n",
      "/var/log/hadoop-yarn/apps/hadoop/logs/application_1620307932889_0002 <dir>\n",
      "/var/log/hadoop-yarn/apps/livy <dir>\n",
      "/var/log/hadoop-yarn/apps/livy/logs <dir>\n",
      "/var/log/hadoop-yarn/apps/livy/logs/application_1620307932889_0001 <dir>\n",
      "/var/log/hadoop-yarn/apps/livy/logs/application_1620307932889_0001/ip-172-31-13-173.ec2.internal_8041 15580 bytes, 1 block(s):  OK\n",
      "0. BP-1596529054-172.31.13.255-1620307919098:blk_1073741838_1014 len=15580 Live_repl=1\n",
      "\n",
      "/var/log/hadoop-yarn/apps/livy/logs/application_1620307932889_0001/ip-172-31-4-27.ec2.internal_8041 27472 bytes, 1 block(s):  OK\n",
      "0. BP-1596529054-172.31.13.255-1620307919098:blk_1073741839_1015 len=27472 Live_repl=1\n",
      "\n",
      "/var/log/spark <dir>\n",
      "/var/log/spark/apps <dir>\n",
      "/var/log/spark/apps/application_1620307932889_0001 50348 bytes, 1 block(s):  OK\n",
      "0. BP-1596529054-172.31.13.255-1620307919098:blk_1073741837_1013 len=50348 Live_repl=1\n",
      "\n",
      "Status: HEALTHY\n",
      " Total size:\t633949859 B (Total open files size: 315 B)\n",
      " Total dirs:\t29\n",
      " Total files:\t11\n",
      " Total symlinks:\t\t0 (Files currently being written: 1)\n",
      " Total blocks (validated):\t13 (avg. block size 48765373 B) (Total open file blocks (not validated): 1)\n",
      " Minimally replicated blocks:\t13 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t1\n",
      " Average block replication:\t1.0769231\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Number of data-nodes:\t\t2\n",
      " Number of racks:\t\t1\n",
      "FSCK ended at Thu May 06 21:54:23 UTC 2021 in 2 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "! hdfs fsck / -files -blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How HDFS looks like on disk (on remote nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp -i ~/andrey.pem ~/andrey.pem hadoop@ec2-3-235-150-104.compute-1.amazonaws.com:~\n"
     ]
    }
   ],
   "source": [
    "# first copy ~/andrey.pem (your ssh key) to master node by running on your machine:\n",
    "print(spark_utils.copy_ssh_key_hint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/05/06 21:54:43 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-13-255.ec2.internal/172.31.13.255:8032\n",
      "Total Nodes:2\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "ip-172-31-13-173.ec2.internal:8041\t        RUNNING\tip-172-31-13-173.ec2.internal:8042\t                           1\n",
      "ip-172-31-4-27.ec2.internal:8041\t        RUNNING\tip-172-31-4-27.ec2.internal:8042\t                           0\n"
     ]
    }
   ],
   "source": [
    "# list all nodes\n",
    "! yarn node -list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hdfs:\r\n",
      "total 4.0K\r\n",
      "drwxr-xr-x 3 hdfs hdfs 70 May  6 13:32 current\r\n",
      "-rw-r--r-- 1 hdfs hdfs 34 May  6 13:32 in_use.lock\r\n",
      "\r\n",
      "/mnt/hdfs/current:\r\n",
      "total 4.0K\r\n",
      "drwx------ 4 hdfs hdfs  54 May  6 13:32 BP-1596529054-172.31.13.255-1620307919098\r\n",
      "-rw-r--r-- 1 hdfs hdfs 229 May  6 13:32 VERSION\r\n",
      "\r\n",
      "/mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098:\r\n",
      "total 4.0K\r\n",
      "drwxr-xr-x 4 hdfs hdfs  49 May  6 13:32 current\r\n",
      "-rw-r--r-- 1 hdfs hdfs 166 May  6 13:32 scanner.cursor\r\n",
      "drwxr-xr-x 2 hdfs hdfs   6 May  6 21:21 tmp\r\n",
      "\r\n",
      "/mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current:\r\n",
      "total 4.0K\r\n",
      "drwxr-xr-x 3 hdfs hdfs  21 May  6 13:42 finalized\r\n",
      "drwxr-xr-x 2 hdfs hdfs   6 May  6 21:54 rbw\r\n",
      "-rw-r--r-- 1 hdfs hdfs 145 May  6 13:32 VERSION\r\n",
      "\r\n",
      "/mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized:\r\n",
      "total 0\r\n",
      "drwxr-xr-x 3 hdfs hdfs 21 May  6 13:42 subdir0\r\n",
      "\r\n",
      "/mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0:\r\n",
      "total 4.0K\r\n",
      "drwxr-xr-x 2 hdfs hdfs 4.0K May  6 21:54 subdir0\r\n",
      "\r\n",
      "/mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0:\r\n",
      "total 609M\r\n",
      "-rw-r--r-- 1 hdfs hdfs  16K May  6 13:48 blk_1073741838\r\n",
      "-rw-r--r-- 1 hdfs hdfs  131 May  6 13:48 blk_1073741838_1014.meta\r\n",
      "-rw-r--r-- 1 hdfs hdfs 128M May  6 14:54 blk_1073741840\r\n",
      "-rw-r--r-- 1 hdfs hdfs 1.1M May  6 14:54 blk_1073741840_1016.meta\r\n",
      "-rw-r--r-- 1 hdfs hdfs  92M May  6 14:54 blk_1073741841\r\n",
      "-rw-r--r-- 1 hdfs hdfs 734K May  6 14:54 blk_1073741841_1017.meta\r\n",
      "-rw-r--r-- 1 hdfs hdfs  42K May  6 14:54 blk_1073741843\r\n",
      "-rw-r--r-- 1 hdfs hdfs  339 May  6 14:54 blk_1073741843_1019.meta\r\n",
      "-rw-r--r-- 1 hdfs hdfs   10 May  6 21:21 blk_1073741846\r\n",
      "-rw-r--r-- 1 hdfs hdfs   11 May  6 21:21 blk_1073741846_1022.meta\r\n",
      "-rw-r--r-- 1 hdfs hdfs    8 May  6 21:19 blk_1073741847\r\n",
      "-rw-r--r-- 1 hdfs hdfs   11 May  6 21:19 blk_1073741847_1023.meta\r\n",
      "-rw-r--r-- 1 hdfs hdfs 128M May  6 21:54 blk_1073741851\r\n",
      "-rw-r--r-- 1 hdfs hdfs 1.1M May  6 21:54 blk_1073741851_1027.meta\r\n",
      "-rw-r--r-- 1 hdfs hdfs 128M May  6 21:54 blk_1073741852\r\n",
      "-rw-r--r-- 1 hdfs hdfs 1.1M May  6 21:54 blk_1073741852_1028.meta\r\n",
      "-rw-r--r-- 1 hdfs hdfs 128M May  6 21:54 blk_1073741853\r\n",
      "-rw-r--r-- 1 hdfs hdfs 1.1M May  6 21:54 blk_1073741853_1029.meta\r\n",
      "\r\n",
      "/mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/rbw:\r\n",
      "total 0\r\n",
      "\r\n",
      "/mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/tmp:\r\n",
      "total 0\r\n"
     ]
    }
   ],
   "source": [
    "! ssh -i ~/andrey.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null hadoop@ip-172-31-13-173.ec2.internal \"sudo bash -c 'ls -lh -R /mnt/hdfs'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741838_1014.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000f��w\u0017b�\r\n",
      ",��X\u0006�\u0011\u0010(C\t�Y1��\"\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741838 <==\r\n",
      "�\u0011�h��׶9�A@���P\t\u0000\u0007VERSION\u0004\u0000\u0000\u0000\u0001\u0011\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741840_1016.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000�u�*��k\u0018\u0015�4���w�\u001E",
      "J@;z8�!�\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741840 <==\r\n",
      "PK\u0003\u0004\u0014\u0000\b\b\b\u0000�v�R\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u001C",
      "\u0000\u0000\u0000ja\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741841_1017.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000�T����Q�Vt�/*�\u0003����հA=�\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741841 <==\r\n",
      "�ؓ\u0011\u0003\u0018rWJ����(\u0012�\u001C",
      "��е]�>�ȳ�A�\u001E",
      "�\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741843_1019.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000\u0015���A��\u001D",
      "u�7�h\u000F�!Θ�����A�\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741843 <==\r\n",
      "PK\u0003\u0004\u0014\u0000\u0000\u0000\b\u0000X��L��\u001A��\u0005\u0000\u0000\u000E\u001A\u0000\u0000\u0010\u0000\u001C",
      "\u0000py\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741847_1023.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000,��\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741847 <==\r\n",
      "1 2 3 4\r\n",
      "\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741846_1022.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000\u000F[H�\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741846 <==\r\n",
      "Test file\r\n",
      "\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741851_1027.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000M]L}&��*���:`\u0001\bf�1\\4M]L}&\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741851 <==\r\n",
      "128M\r\n",
      "128M\r\n",
      "128M\r\n",
      "128M\r\n",
      "128M\r\n",
      "128M\r\n",
      "12\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741852_1028.meta <==\r\n",
      "\\�Î�An\u0013��E_���e\b\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741852 <==\r\n",
      "256M\r\n",
      "256M\r\n",
      "256M\r\n",
      "256M\r\n",
      "256M\r\n",
      "256M\r\n",
      "25\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741853_1029.meta <==\r\n",
      "\\�Î�An\u0013��E_�\r\n",
      "==> /mnt/hdfs/current/BP-1596529054-172.31.13.255-1620307919098/current/finalized/subdir0/subdir0/blk_1073741853 <==\r\n",
      "M\r\n",
      "256M\r\n",
      "256M\r\n",
      "256M\r\n",
      "256M\r\n",
      "256M\r\n",
      "256M\r\n"
     ]
    }
   ],
   "source": [
    "! ssh -i ~/andrey.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null hadoop@ip-172-31-13-173.ec2.internal \"sudo bash -c 'find /mnt/hdfs | grep blk | xargs head -c 32'\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}